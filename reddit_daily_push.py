#!/usr/bin/env python3
"""
Reddit æ¯æ—¥æ¨é€è„šæœ¬
æ¯å¤©ä¸­åˆ12ç‚¹è·å–æŒ‡å®š subreddit å†…å®¹å¹¶é€šè¿‡ Feishu æ¨é€
"""

import os
import sys
import json
import time
import schedule
import requests
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import xml.etree.ElementTree as ET
from pathlib import Path

# æ·»åŠ é¡¹ç›®ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(project_root / 'reddit_push.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RedditFetcher:
    """Reddit å†…å®¹è·å–å™¨"""
    
    def __init__(self, proxy_url: str = "http://127.0.0.1:7890"):
        """
        åˆå§‹åŒ– Reddit è·å–å™¨
        
        Args:
            proxy_url: Clash ä»£ç†åœ°å€
        """
        self.proxy_url = proxy_url
        self.proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        # é»˜è®¤å…³æ³¨çš„ subreddit
        self.default_subreddits = [
            'programming',
            'technology',
            'python',
            'webdev',
            'linux',
            'opensource'
        ]
        
        logger.info(f"RedditFetcher åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨ä»£ç†: {proxy_url}")
    
    def fetch_subreddit_rss(self, subreddit: str, limit: int = 10) -> List[Dict]:
        """
        è·å– subreddit çš„ RSS å†…å®¹
        
        Args:
            subreddit: subreddit åç§°
            limit: æœ€å¤§å¸–å­æ•°é‡
            
        Returns:
            å¸–å­åˆ—è¡¨
        """
        url = f"https://www.reddit.com/r/{subreddit}/.rss"
        
        try:
            logger.info(f"è·å– subreddit: r/{subreddit}")
            response = requests.get(
                url,
                proxies=self.proxies,
                headers=self.headers,
                timeout=15
            )
            
            if response.status_code == 200:
                return self.parse_rss_content(response.text, subreddit, limit)
            else:
                logger.error(f"è·å–å¤±è´¥: HTTP {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"è·å– subreddit r/{subreddit} æ—¶å‡ºé”™: {e}")
            return []
    
    def parse_rss_content(self, xml_content: str, subreddit: str, limit: int) -> List[Dict]:
        """
        è§£æ RSS XML å†…å®¹
        
        Args:
            xml_content: RSS XML å†…å®¹
            subreddit: subreddit åç§°
            limit: æœ€å¤§å¸–å­æ•°é‡
            
        Returns:
            è§£æåçš„å¸–å­åˆ—è¡¨
        """
        try:
            # è§£æ XML
            root = ET.fromstring(xml_content)
            
            # Reddit RSS ä½¿ç”¨ Atom æ ¼å¼ï¼ŒæŸ¥æ‰¾æ‰€æœ‰ entry å…ƒç´ 
            namespace = {'atom': 'http://www.w3.org/2005/Atom'}
            
            # å°è¯•ä¸åŒçš„å…ƒç´ æŸ¥æ‰¾æ–¹å¼
            items = []
            
            # æ–¹æ³•1: æŸ¥æ‰¾ Atom entry å…ƒç´ 
            items = root.findall('.//{http://www.w3.org/2005/Atom}entry')
            
            # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å‘½åç©ºé—´
            if not items:
                items = root.findall('.//entry')
            
            # æ–¹æ³•3: å°è¯• item å…ƒç´ ï¼ˆä¼ ç»Ÿ RSSï¼‰
            if not items:
                items = root.findall('.//item')
            
            posts = []
            for item in items[:limit]:
                # æå–æ ‡é¢˜
                title_elem = item.find('{http://www.w3.org/2005/Atom}title')
                if title_elem is None:
                    title_elem = item.find('title')
                
                # æå–é“¾æ¥
                link_elem = item.find('{http://www.w3.org/2005/Atom}link')
                if link_elem is None:
                    link_elem = item.find('link')
                
                # æå–ä½œè€…
                author_elem = item.find('{http://www.w3.org/2005/Atom}author')
                if author_elem is not None:
                    name_elem = author_elem.find('{http://www.w3.org/2005/Atom}name')
                    author = name_elem.text if name_elem is not None else ''
                else:
                    author_elem = item.find('author')
                    author = author_elem.text if author_elem is not None else ''
                
                # æå–å‘å¸ƒæ—¶é—´
                published_elem = item.find('{http://www.w3.org/2005/Atom}published')
                if published_elem is None:
                    published_elem = item.find('pubDate')
                
                # æå–å†…å®¹
                content_elem = item.find('{http://www.w3.org/2005/Atom}content')
                if content_elem is None:
                    content_elem = item.find('description')
                
                post = {
                    'subreddit': subreddit,
                    'title': title_elem.text if title_elem is not None else 'æ— æ ‡é¢˜',
                    'link': link_elem.get('href') if link_elem is not None and 'href' in link_elem.attrib else (link_elem.text if link_elem is not None else ''),
                    'description': content_elem.text if content_elem is not None else '',
                    'author': author,
                    'pub_date': published_elem.text if published_elem is not None else '',
                    'guid': '',
                    'fetched_at': datetime.now().isoformat()
                }
                
                # æ¸…ç†æè¿°æ–‡æœ¬
                if post['description']:
                    # ç§»é™¤ HTML æ ‡ç­¾
                    import re
                    post['description'] = re.sub(r'<[^>]+>', '', post['description'])
                    post['description'] = post['description'][:200] + '...' if len(post['description']) > 200 else post['description']
                
                # å¦‚æœé“¾æ¥ä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–å±æ€§è·å–
                if not post['link'] and link_elem is not None:
                    # å°è¯•è·å– href å±æ€§
                    post['link'] = link_elem.get('href', '')
                
                posts.append(post)
            
            logger.info(f"è§£æåˆ° {len(posts)} ä¸ªå¸–å­æ¥è‡ª r/{subreddit}")
            return posts
            
        except Exception as e:
            logger.error(f"è§£æ RSS å†…å®¹æ—¶å‡ºé”™: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    def _get_element_text(self, element, tag_name: str) -> str:
        """å®‰å…¨è·å–å…ƒç´ æ–‡æœ¬"""
        try:
            elem = element.find(tag_name)
            if elem is not None:
                return elem.text or ''
        except:
            pass
        return ''
    
    def fetch_multiple_subreddits(self, subreddits: List[str], limit_per_sub: int = 5) -> Dict[str, List[Dict]]:
        """
        è·å–å¤šä¸ª subreddit çš„å†…å®¹
        
        Args:
            subreddits: subreddit åç§°åˆ—è¡¨
            limit_per_sub: æ¯ä¸ª subreddit çš„æœ€å¤§å¸–å­æ•°
            
        Returns:
            æŒ‰ subreddit åˆ†ç»„çš„å¸–å­å­—å…¸
        """
        results = {}
        
        for subreddit in subreddits:
            posts = self.fetch_subreddit_rss(subreddit, limit_per_sub)
            if posts:
                results[subreddit] = posts
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        return results
    
    def get_trending_posts(self, subreddits: Optional[List[str]] = None, 
                          total_limit: int = 15) -> List[Dict]:
        """
        è·å–çƒ­é—¨å¸–å­ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
        
        Args:
            subreddits: è¦è·å–çš„ subreddit åˆ—è¡¨ï¼ŒNone åˆ™ä½¿ç”¨é»˜è®¤
            total_limit: æ€»å¸–å­æ•°é‡é™åˆ¶
            
        Returns:
            åˆå¹¶åçš„çƒ­é—¨å¸–å­åˆ—è¡¨
        """
        if subreddits is None:
            subreddits = self.default_subreddits
        
        all_posts = []
        results = self.fetch_multiple_subreddits(subreddits, limit_per_sub=3)
        
        for subreddit, posts in results.items():
            for post in posts:
                post['score'] = self._calculate_post_score(post)
                all_posts.append(post)
        
        # æŒ‰åˆ†æ•°æ’åº
        all_posts.sort(key=lambda x: x['score'], reverse=True)
        
        return all_posts[:total_limit]
    
    def _calculate_post_score(self, post: Dict) -> int:
        """è®¡ç®—å¸–å­åˆ†æ•°ï¼ˆç®€å•å®ç°ï¼‰"""
        score = 0
        
        # æ ‡é¢˜é•¿åº¦é€‚ä¸­åŠ åˆ†
        title_len = len(post.get('title', ''))
        if 20 <= title_len <= 100:
            score += 10
        
        # æè¿°é•¿åº¦åŠ åˆ†
        desc_len = len(post.get('description', ''))
        if desc_len > 50:
            score += 5
        
        # ç‰¹å®šå…³é”®è¯åŠ åˆ†
        keywords = ['python', 'tutorial', 'guide', 'news', 'update', 'release']
        title = post.get('title', '').lower()
        for keyword in keywords:
            if keyword in title:
                score += 3
        
        return score

class FeishuNotifier:
    """é£ä¹¦æ¶ˆæ¯é€šçŸ¥å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é£ä¹¦é€šçŸ¥å™¨"""
        # è¿™é‡Œéœ€è¦é…ç½®é£ä¹¦çš„ webhook æˆ– API
        # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿå‘é€ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦é…ç½®çœŸå® webhook
        self.webhook_url = os.environ.get('FEISHU_WEBHOOK_URL', '')
        self.enabled = bool(self.webhook_url)
        
        if self.enabled:
            logger.info("é£ä¹¦é€šçŸ¥å™¨å·²å¯ç”¨")
        else:
            logger.warning("é£ä¹¦é€šçŸ¥å™¨æœªé…ç½®ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿå‘é€")
    
    def send_message(self, title: str, content: str, posts: List[Dict] = None) -> bool:
        """
        å‘é€æ¶ˆæ¯åˆ°é£ä¹¦
        
        Args:
            title: æ¶ˆæ¯æ ‡é¢˜
            content: æ¶ˆæ¯å†…å®¹
            posts: å¸–å­åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ˜¯å¦å‘é€æˆåŠŸ
        """
        try:
            if self.enabled:
                return self._send_real_message(title, content, posts)
            else:
                return self._send_mock_message(title, content, posts)
                
        except Exception as e:
            logger.error(f"å‘é€é£ä¹¦æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            return False
    
    def _send_real_message(self, title: str, content: str, posts: List[Dict]) -> bool:
        """å‘é€çœŸå®é£ä¹¦æ¶ˆæ¯"""
        # æ„å»ºé£ä¹¦æ¶ˆæ¯å¡ç‰‡
        message_card = {
            "msg_type": "interactive",
            "card": {
                "config": {
                    "wide_screen_mode": True
                },
                "header": {
                    "title": {
                        "tag": "plain_text",
                        "content": title
                    },
                    "template": "blue"
                },
                "elements": []
            }
        }
        
        # æ·»åŠ å†…å®¹
        message_card["card"]["elements"].append({
            "tag": "div",
            "text": {
                "tag": "lark_md",
                "content": content
            }
        })
        
        # æ·»åŠ å¸–å­åˆ—è¡¨
        if posts:
            for i, post in enumerate(posts[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                post_element = {
                    "tag": "div",
                    "text": {
                        "tag": "lark_md",
                        "content": f"{i}. **{post.get('title', 'æ— æ ‡é¢˜')}**\n"
                                  f"   ğŸ“ r/{post.get('subreddit', 'unknown')}\n"
                                  f"   ğŸ”— [æŸ¥çœ‹åŸæ–‡]({post.get('link', '#')})"
                    },
                    "fields": []
                }
                message_card["card"]["elements"].append(post_element)
        
        # æ·»åŠ æ—¶é—´æˆ³
        message_card["card"]["elements"].append({
            "tag": "note",
            "elements": [{
                "tag": "plain_text",
                "content": f"ğŸ“… æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            }]
        })
        
        # å‘é€è¯·æ±‚
        try:
            response = requests.post(
                self.webhook_url,
                json=message_card,
                timeout=10
            )
            
            if response.status_code == 200:
                logger.info("é£ä¹¦æ¶ˆæ¯å‘é€æˆåŠŸ")
                return True
            else:
                logger.error(f"é£ä¹¦æ¶ˆæ¯å‘é€å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"å‘é€é£ä¹¦è¯·æ±‚æ—¶å‡ºé”™: {e}")
            return False
    
    def _send_mock_message(self, title: str, content: str, posts: List[Dict]) -> bool:
        """æ¨¡æ‹Ÿå‘é€æ¶ˆæ¯ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        logger.info("=" * 60)
        logger.info(f"ğŸ“¨ æ¨¡æ‹Ÿå‘é€é£ä¹¦æ¶ˆæ¯")
        logger.info(f"æ ‡é¢˜: {title}")
        logger.info(f"å†…å®¹: {content}")
        
        if posts:
            logger.info("çƒ­é—¨å¸–å­:")
            for i, post in enumerate(posts[:5], 1):
                logger.info(f"  {i}. {post.get('title', 'æ— æ ‡é¢˜')}")
                logger.info(f"     æ¥æº: r/{post.get('subreddit', 'unknown')}")
                logger.info(f"     é“¾æ¥: {post.get('link', '#')}")
        
        logger.info("=" * 60)
        
        # åŒæ—¶ä¿å­˜åˆ°æ–‡ä»¶
        self._save_to_file(title, content, posts)
        
        return True
    
    def _save_to_file(self, title: str, content: str, posts: List[Dict]):
        """ä¿å­˜æ¶ˆæ¯åˆ°æ–‡ä»¶"""
        try:
            output_dir = project_root / 'reddit_output'
            output_dir.mkdir(exist_ok=True)
            
            filename = output_dir / f"reddit_push_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"æ ‡é¢˜: {title}\n")
                f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"å†…å®¹: {content}\n\n")
                
                if posts:
                    f.write("çƒ­é—¨å¸–å­:\n")
                    for i, post in enumerate(posts, 1):
                        f.write(f"\n{i}. {post.get('title', 'æ— æ ‡é¢˜')}\n")
                        f.write(f"   æ¥æº: r/{post.get('subreddit', 'unknown')}\n")
                        f.write(f"   é“¾æ¥: {post.get('link', '#')}\n")
                        if post.get('description'):
                            f.write(f"   æè¿°: {post.get('description')}\n")
            
            logger.info(f"æ¶ˆæ¯å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ°æ–‡ä»¶æ—¶å‡ºé”™: {e}")

class RedditDailyPusher:
    """Reddit æ¯æ—¥æ¨é€ä¸»ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¨é€å™¨"""
        self.reddit_fetcher = RedditFetcher()
        self.feishu_notifier = FeishuNotifier()
        
        # é…ç½®
        self.subreddits = [
            'programming',
            'technology', 
            'python',
            'webdev',
            'linux',
            'opensource'
        ]
        
        self.push_time = "12:00"  # æ¯å¤©ä¸­åˆ12ç‚¹æ¨é€
        
        logger.info(f"RedditDailyPusher åˆå§‹åŒ–å®Œæˆï¼Œæ¨é€æ—¶é—´: {self.push_time}")
    
    def fetch_and_push(self):
        """è·å–å¹¶æ¨é€å†…å®¹"""
        try:
            logger.info("å¼€å§‹è·å– Reddit å†…å®¹...")
            
            # è·å–çƒ­é—¨å¸–å­
            trending_posts = self.reddit_fetcher.get_trending_posts(
                self.subreddits,
                total_limit=10
            )
            
            if not trending_posts:
                logger.warning("æœªè·å–åˆ°ä»»ä½•å¸–å­å†…å®¹")
                return False
            
            # æ„å»ºæ¶ˆæ¯å†…å®¹
            title = f"ğŸ“° Reddit æŠ€æœ¯èµ„è®¯æ—¥æŠ¥ - {datetime.now().strftime('%Y-%m-%d')}"
            
            subreddit_counts = {}
            for post in trending_posts:
                sub = post.get('subreddit', 'unknown')
                subreddit_counts[sub] = subreddit_counts.get(sub, 0) + 1
            
            stats_text = " | ".join([f"r/{sub}: {count}" for sub, count in subreddit_counts.items()])
            
            content = f"""ğŸš€ ä»Šæ—¥çƒ­é—¨æŠ€æœ¯å†…å®¹å·²é€è¾¾ï¼

**ç»Ÿè®¡æ‘˜è¦:**
{stats_text}

**ç²¾é€‰å†…å®¹:** (å…± {len(trending_posts)} æ¡)
"""
            
            # å‘é€æ¶ˆæ¯
            success = self.feishu_notifier.send_message(title, content, trending_posts)
            
            if success:
                logger.info("Reddit å†…å®¹æ¨é€æˆåŠŸ")
                # ä¿å­˜æ¨é€è®°å½•
                self._save_push_record(trending_posts)
            else:
                logger.error("Reddit å†…å®¹æ¨é€å¤±è´¥")
            
            return success
            
        except Exception as e:
            logger.error(f"è·å–å¹¶æ¨é€å†…å®¹æ—¶å‡ºé”™: {e}")
            return False
    
    def _save_push_record(self, posts: List[Dict]):
        """ä¿å­˜æ¨é€è®°å½•"""
        try:
            record = {
                'push_time': datetime.now().isoformat(),
                'post_count': len(posts),
                'subreddits': list(set(p['subreddit'] for p in posts)),
                'posts': posts
            }
            
            records_dir = project_root / 'reddit_records'
            records_dir.mkdir(exist_ok=True)
            
            filename = records_dir / f"push_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(record, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ¨é€è®°å½•å·²ä¿å­˜: {filename}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨é€è®°å½•æ—¶å‡ºé”™: {e}")
    
    def run_daily(self):
        """è¿è¡Œæ¯æ—¥æ¨é€ä»»åŠ¡"""
        logger.info(f"å¯åŠ¨æ¯æ—¥æ¨é€æœåŠ¡ï¼Œæ¨é€æ—¶é—´: {self.push_time}")
        
        # å®‰æ’æ¯æ—¥ä»»åŠ¡
        schedule.every().day.at(self.push_time).do(self.fetch_and_push)
        
        # ç«‹å³è¿è¡Œä¸€æ¬¡ï¼ˆæµ‹è¯•ç”¨ï¼‰
        logger.info("ç«‹å³è¿è¡Œä¸€æ¬¡æµ‹è¯•æ¨é€...")
        self.fetch_and_push()
        
        # ä¿æŒè¿è¡Œ
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢æœåŠ¡")
                break
            except Exception as e:
                logger.error(f"è°ƒåº¦å™¨è¿è¡Œæ—¶å‡ºé”™: {e}")
                time.sleep(300)  # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿ
    
    def run_once(self):
        """è¿è¡Œä¸€æ¬¡ï¼ˆæ‰‹åŠ¨è§¦å‘ï¼‰"""
        logger.info("æ‰‹åŠ¨è§¦å‘å•æ¬¡æ¨é€...")
        return self.fetch_and_push()

def setup_cron_job():
    """è®¾ç½® cron å®šæ—¶ä»»åŠ¡"""
    cron_content = f"""# Reddit æ¯æ—¥æ¨é€ä»»åŠ¡
# æ¯å¤©ä¸­åˆ12ç‚¹è¿è¡Œ
0 12 * * * cd {project_root} && {sys.executable} {__file__} --once >> {project_root}/reddit_cron.log 2>&1

# æµ‹è¯•ä»»åŠ¡ï¼ˆæ¯5åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ï¼‰
# */5 * * * * cd {project_root} && {sys.executable} {__file__} --once >> {project_root}/reddit_test.log 2>&1
"""
    
    cron_file = project_root / 'reddit_cron.txt'
    with open(cron_file, 'w') as f:
        f.write(cron_content)
    
    logger.info(f"Cron é…ç½®å·²ä¿å­˜åˆ°: {cron_file}")
    logger.info("è¯·æ‰‹åŠ¨æ·»åŠ åˆ° crontab: crontab -e")
    
    # è¿”å›å®‰è£…è¯´æ˜
    install_guide = f"""
    ğŸ“… Cron ä»»åŠ¡å®‰è£…è¯´æ˜:
    1. ç¼–è¾‘ crontab: crontab -e
    2. æ·»åŠ ä»¥ä¸‹è¡Œ:
    
    {cron_content}
    
    3. ä¿å­˜å¹¶é€€å‡º
    4. éªŒè¯: crontab -l
    """
    
    return install_guide